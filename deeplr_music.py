#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
music_gen_deeplearning_advanced.py

æœ¬ç³»ç»Ÿå®ç°äº†ä»å›¾åƒè‡ªåŠ¨ç”ŸæˆéŸ³ä¹åŠä¹è°±çš„å…¨æµç¨‹ï¼š
1. åˆ©ç”¨ ResNet18 æå– 512 ç»´æ·±åº¦ç‰¹å¾ï¼Œå¹¶æ ¹æ®ç‰¹å¾å‡å€¼å†³å®šè°ƒæ€§ã€æ¨¡å¼å’ŒèŠ‚å¥é€Ÿåº¦ã€‚
2. åŒæ—¶æå–å›¾åƒ HSV å‡å€¼ï¼Œé€šè¿‡ä¸¤ç§è‰²å½©æ˜ å°„å‡½æ•°ç¡®å®šè°ƒæ€§ï¼ˆå‚è€ƒ Scriabin/Kandinsky çš„è‰²å½©-è°ƒæ€§å¯¹åº”ï¼‰ã€‚
3. åˆ©ç”¨ Markov é“¾ç”Ÿæˆå’Œå¼¦åºåˆ—ï¼Œå¹¶é€šè¿‡ä¸¤ç§ç”Ÿæˆæ–¹æ³•ï¼š
   - "dual" æ¨¡å¼ï¼šé‡‡ç”¨å¯¹ä½è§„åˆ™ç”Ÿæˆå·¦å³å£°éƒ¨ï¼ˆä½¿ç”¨ generate_dual_voice_measureï¼‰ï¼Œ
   - "pattern" æ¨¡å¼ï¼šé‡‡ç”¨é¢„å®šä¹‰ä¼´å¥æ¨¡å¼ï¼ˆgenerate_accompanimentï¼‰ç”Ÿæˆå·¦å£°éƒ¨ï¼Œè€Œå³å£°éƒ¨ä»é‡‡ç”¨å¯¹ä½æ–¹æ³•ç”Ÿæˆã€‚
4. æœ€ç»ˆç”Ÿæˆ MIDI æ–‡ä»¶ã€è½¬æ¢ä¸º LilyPond è®°è°±æ–‡æœ¬ï¼Œå¹¶è°ƒç”¨ LilyPond ç”Ÿæˆ PDF ä¹è°±ã€‚
5. å…³é”®å‚æ•°ï¼ˆimg_pathã€lengthã€methodã€pattern_nameã€left_program_indexã€right_program_indexï¼‰å‡å¯ç”±å¤–éƒ¨è‡ªå®šä¹‰ä¼ å…¥ã€‚

ä½œè€…ï¼šYao  
"""

import os
import cv2
import random
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as T
import torchvision.models as models
from torchvision.models import MobileNet_V2_Weights
import subprocess
import pretty_midi
import hashlib




#########################################################
# A. æ·±åº¦ç‰¹å¾æå–ä¸ç®€å•åˆ†ç±»/å›å½’ (ç¤ºä¾‹)
#########################################################

def load_resnet18_model():
    """
    åŠ è½½é¢„è®­ç»ƒçš„ ResNet18, å°† fc æ›¿æ¢ä¸º Identity, è¾“å‡º 512 ç»´ç‰¹å¾å‘é‡
    """
    model = models.resnet18(pretrained=True)
    model.fc = nn.Identity()
    model.eval()
    scripted_model = torch.jit.script(model)
    return scripted_model

def load_mobilenet_v2():
    model = models.mobilenet_v2()
    # model = models.mobilenet_v2(weights='DEFAULT')
    model.load_state_dict(torch.load("mobilenet_v2.pth", map_location="cpu"))
    model.classifier = nn.Identity()
    model.eval()
    scripted_model = torch.jit.script(model)
    return scripted_model

def extract_deep_features_bgr(image_bgr, model):
    """
    ä» BGR å›¾ç‰‡æå– 512 ç»´æ·±åº¦ç‰¹å¾ã€‚
    """
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    image_rgb = cv2.resize(image_rgb, (224, 224))
    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])
    tensor_img = transform(image_rgb).unsqueeze(0)  # (1,3,224,224)
    with torch.no_grad():
        feats = model(tensor_img)  # (1,512)
    vec = feats.squeeze(0).numpy()  # (512,)
    return vec

def decide_deep_params(deep_vec):
    """
    åˆ©ç”¨12ä¸ªè°ƒæ€§ (C, C#, D, D#, E, F, F#, G, G#, A, A#, B) ä½œä¸º cluster centerï¼Œ
    ç”¨æ¬§å¼è·ç¦»é€‰æ‹©æœ€åˆé€‚çš„ root_noteï¼›
    è‹¥ dv_mean > 0 åˆ™ä¸º majorï¼Œå¦åˆ™ä¸º minorï¼›
    åŒæ—¶é‡‡ç”¨ dv_mean çš„çº¿æ€§æ˜ å°„è®¡ç®— tempoï¼š
      factor = (dv_mean + 2) / 4ï¼Œtempo = 60 + 80 * factorï¼Œ
    å…¶ä¸­ tempo èŒƒå›´ä¸º [60, 140] BPMã€‚
    """
    all_keys = ["C","C#","D","D#","E","F","F#","G","G#","A","A#","B"]
    cluster_centers = {}
    offsets = np.linspace(-0.5, 0.5, len(all_keys))
    for i, k in enumerate(all_keys):
        center = np.random.randn(1280) * 0.1 + offsets[i]
        cluster_centers[k] = center

    best_key = None
    best_dist = 1e9
    for k, center in cluster_centers.items():
        dist = np.linalg.norm(deep_vec - center)
        if dist < best_dist:
            best_dist = dist
            best_key = k

    dv_mean = np.mean(deep_vec)
    scale_mode = "major" if dv_mean > 0 else "minor"
    factor = (dv_mean + 2) / 4
    factor = max(0, min(1, factor))
    tempo = 60 + int(80 * factor)
    return best_key, scale_mode, tempo

#########################################################
# B. HSV ä¸è‰²å½©æ˜ å°„
#########################################################

def color_to_tonality(h_mean, s_mean, v_mean):
    """
    å°† hue (0~180) ç­‰åˆ†ä¸º12ä¸ªåŒºé—´ï¼ˆæ¯15åº¦ï¼‰ï¼Œè¿”å›å¯¹åº”çš„è°ƒæ€§ï¼ˆä¾‹å¦‚ "C", "C#", "D", ...ï¼‰ã€‚
    """
    all_keys = ["C","C#","D","D#","E","F","F#","G","G#","A","A#","B"]
    idx = int(h_mean // 15)
    if idx >= 12:
        idx = 11
    return all_keys[idx]

def color_to_tonality_new(h_mean, s_mean, v_mean):
    """
    æ ¹æ® Scriabin æ‰€å€¡å¯¼çš„åäºŒå¤§è°ƒè‰²å½©åˆ†é…ï¼š
      æ¯15åº¦ä¸€ä¸ªåŒºé—´ï¼Œè¿”å›ä¸€ä¸ªä¸‰å…ƒç»„ï¼š(key, color_desc, mystic_set)
    ä¾‹å¦‚ï¼š
      0~15   -> ("C", "red, plain", [0,6,10,4,9,2])
      15~30  -> ("G", "orange (red-yellow, fiery)", [7,1,5,11,4,9])
      ...
      165~180-> ("F", "dark red", [5,11,3,9,2,7])
    """
    data = [
        (15, "C",  "red, plain",                [0,6,10,4,9,2]),
        (30, "G",  "orange (red-yellow, fiery)",[7,1,5,11,4,9]),
        (45, "D",  "sunny yellow",              [2,8,0,6,11,4]),
        (60, "A",  "green, grass green",        [9,3,7,1,6,11]),
        (75, "E",  "blue greenish (light blue)",[4,10,2,8,1,6]),
        (90, "B",  "dark blue with light blue", [11,5,9,3,8,1]),
        (105,"F#", "dark blue with violet shade", [6,0,4,10,3,8]),
        (120,"Db", "pure violet",               [1,7,11,5,10,3]),
        (135,"Ab", "lily-colored, reddish",     [8,2,6,0,5,10]),
        (150,"Eb", "steely blue, metallic",     [3,9,1,7,0,5]),
        (165,"Bb", "metallic leaden grey",      [10,4,8,2,7,0]),
        (180,"F",  "dark red",                  [5,11,3,9,2,7]),
    ]
    for boundary, key, color_desc, mystic_set in data:
        if h_mean < boundary:
            return key, color_desc, mystic_set
    return "F", "dark red", [5,11,3,9,2,7]

#########################################################
# C. éšæœºæ€§è®¾ç½®ï¼ˆç¡®ä¿åŒä¸€å›¾åƒç”Ÿæˆç›¸åŒç»“æœï¼‰
#########################################################

def set_deterministic_seed(deep_vec):
    """
    åˆ©ç”¨ deep_vec çš„å­—èŠ‚å“ˆå¸Œå€¼è®¾ç½® random.seedï¼Œç¡®ä¿åŒä¸€å›¾åƒäº§ç”Ÿç›¸åŒçš„éšæœºåºåˆ—
    """
    # ä½¿ç”¨ hashlib.md5 å¾—åˆ°ä¸€ä¸ªç¡®å®šæ€§çš„ 128 ä½å“ˆå¸Œå€¼
    h = hashlib.md5(deep_vec.tobytes()).hexdigest()
    # å–å‰ 8 ä½è½¬ä¸º intï¼ˆ32 ä½ç§å­ï¼‰
    s = int(h[:8], 16)
    random.seed(s)
    np.random.seed(s)

#########################################################
# D. å¯¹ä½ä¸å’Œå¼¦ç”Ÿæˆç›¸å…³å‡½æ•°
#########################################################

# ä¼ ç»ŸåŠŸèƒ½å’Œå¼¦æ˜ å°„
major_map = {
    "I": [0, 4, 7],
    "ii": [2, 5, 9],
    "iii": [4, 7, 11],
    "IV": [5, 9, 0],
    "V": [7, 11, 2],
    "vi": [9, 0, 4],
    "viiÂ°": [11, 2, 5]
}
minor_map = {
    "i": [0, 3, 7],
    "iiÂ°": [2, 5, 8],
    "III": [4, 7, 11],
    "iv": [5, 8, 0],
    "v": [7, 10, 2],
    "VI": [8, 0, 3],
    "VII": [10, 2, 5]
}

NOTE_BASE_MAP = {
    "C": 0, "C#": 1, "Db": 1, "D": 2, "D#": 3, "Eb": 3, "E": 4,
    "F": 5, "F#": 6, "Gb": 6, "G": 7, "G#": 8, "Ab": 8,
    "A": 9, "A#": 10, "Bb": 10, "B": 11, "Cb": 11
}

def chord_pcs_in_scale(chord_label, root_note, scale_mode="major", use_scriabin=False):
    """
    ç”Ÿæˆå’Œå¼¦éŸ³é›†åˆï¼š
      - å½“ use_scriabin ä¸º True æ—¶ï¼Œç›´æ¥è¿”å› Scriabin çš„ mystic chord é›†åˆï¼›
      - å¦åˆ™é‡‡ç”¨ä¼ ç»Ÿçš„ major_map/minor_mapï¼Œå¹¶è¿›è¡Œç§»è°ƒã€‚
    """
    scriabin_sets = {
        "C":  [0,6,10,4,9,2],
        "G":  [7,1,5,11,4,9],
        "D":  [2,8,0,6,11,4],
        "A":  [9,3,7,1,6,11],
        "E":  [4,10,2,8,1,6],
        "B":  [11,5,9,3,8,1],
        "F#": [6,0,4,10,3,8],
        "Db": [1,7,11,5,10,3],
        "Ab": [8,2,6,0,5,10],
        "Eb": [3,9,1,7,0,5],
        "Bb": [10,4,8,2,7,0],
        "F":  [5,11,3,9,2,7]
    }
    if use_scriabin:
        return scriabin_sets.get(root_note, [0,6,10,4,9,2])
    else:
        if scale_mode == "major":
            base_ints = major_map.get(chord_label, [0, 4, 7])
        else:
            base_ints = minor_map.get(chord_label, [0, 3, 7])
        root_val = NOTE_BASE_MAP.get(root_note, 0)
        return [(root_val + i) % 12 for i in base_ints]

def build_markov_states(scale_mode):
    if scale_mode == "major":
        return ["I", "ii", "iii", "IV", "V", "vi", "viiÂ°"]
    else:
        return ["i", "iiÂ°", "III", "iv", "v", "VI", "VII"]

def build_markov_transition(states, s_mean, v_mean):
    base_trans = {}
    for st in states:
        base_trans[st] = {}
        for nxt in states:
            base_trans[st][nxt] = 0.1
        if "V" in base_trans[st]:
            base_trans[st]["V"] += 0.2 * (s_mean / 255)
        if "vi" in base_trans[st]:
            base_trans[st]["vi"] += 0.1 * (1 - v_mean / 255)
    for st in states:
        total = sum(base_trans[st].values())
        for nxt in states:
            base_trans[st][nxt] /= total
    return base_trans

def generate_chord_sequence(states, transition, length=8):
    current = random.choice(states)
    seq = [current]
    for _ in range(length - 1):
        probs = transition[current]
        nxt = random.choices(list(probs.keys()), list(probs.values()))[0]
        seq.append(nxt)
        current = nxt
    return seq

def generate_dual_voice_measure(chord_pcs, beats=4, last_interval=None, hist_notes=(None, None), max_jump=7, step_threshold=2):
    """
    ä¸ºä¸€ä¸ªå°èŠ‚ç”Ÿæˆå·¦å³å£°éƒ¨éŸ³ç¬¦ï¼Œå¹¶éµå¾ªå¯¹ä½è§„åˆ™ï¼š
      - é¿å…å¹³è¡Œäº”åº¦/å…«åº¦ï¼›
      - å¤§è·³åæ¥å‘çº§è¿›è¡Œï¼ˆè¦æ±‚åå‘ä¸”æ­¥å¹…ä¸è¶…è¿‡ 2ï¼‰ã€‚
    è¿”å› (right_list, left_list, new_interval, new_hist)ã€‚
    """
    right_list = []
    left_list = []
    second_last_r, last_r = hist_notes

    # å·¦å£°éƒ¨é‡‡ç”¨ä¼ ç»Ÿæ–¹æ¡ˆï¼šè¿™é‡Œä½ å¯ä»¥æ”¹ä¸ºè°ƒç”¨ pick_bass_in_chord å®ç°å¤šæ ·åŒ–
    bass_pc = chord_pcs[0]
    bass_midi = 12 * (2 + 1) + bass_pc  # octave = 2

    current_interval = last_interval
    current_second_last = second_last_r
    current_last_r = last_r

    for _ in range(beats):
        trials = 0
        candidate_midi = None
        while trials < 30:
            pc = random.choice(chord_pcs)
            octv = 4 if random.random() < 0.85 else 5
            mel_midi = 12 * (octv + 1) + pc
            new_interval = (bass_midi, mel_midi)
            # è¿™é‡Œè°ƒç”¨ is_parallel_perfect ä¸ check_big_leap_directionï¼ˆéœ€ä¿è¯è¿™ä¸¤ä¸ªå‡½æ•°å·²å®ç°ï¼‰
            # ç®€å•æ¨¡æ‹Ÿæ£€æŸ¥
            if current_interval is not None and abs(new_interval[1]-new_interval[0]) in (7,12):
                trials += 1
                continue

            # æ£€æŸ¥ä¸å‰ä¸€ä¸ªå³å£°éƒ¨éŸ³ç¬¦çš„è·³è·ƒå¹…åº¦ï¼Œè‹¥å·²æœ‰å‰éŸ³ï¼Œåˆ™è¦æ±‚ä¸è¶…è¿‡max_jump
            if current_last_r is not None and abs(mel_midi - current_last_r) > max_jump:
                trials += 1
                continue

            candidate_midi = mel_midi
            break

        if candidate_midi is None:
            if current_last_r is not None:
                # æœç´¢å½“å‰å’Œå¼¦åœ¨è¾ƒä½å’Œè¾ƒé«˜å…«åº¦ä¸­ä¸å‰ä¸€ä¸ªéŸ³æ¥è¿‘çš„å€™é€‰éŸ³
                possible_candidates = []
                for pc in chord_pcs:
                    for octv in [4, 5]:
                        candidate = 12 * (octv + 1) + pc
                        if abs(candidate - current_last_r) <= max_jump:
                            possible_candidates.append(candidate)
                if possible_candidates:
                    # é€‰æ‹©ä¸å‰ä¸€ä¸ªéŸ³è·ç¦»æœ€å°çš„å€™é€‰
                    candidate_midi = min(possible_candidates, key=lambda x: abs(x - current_last_r))
                else:
                    candidate_midi = bass_midi + 12  # æœ€åå¤‡ç”¨
            else:
                candidate_midi = bass_midi + 12

        right_list.append(candidate_midi)
        left_list.append(bass_midi)

        current_second_last = current_last_r
        current_last_r = candidate_midi
        current_interval = (bass_midi, candidate_midi)

    return right_list, left_list, current_interval, (current_second_last, current_last_r)

#########################################################
# E. ä¼´å¥æ¨¡å¼ç›¸å…³å‡½æ•°
#########################################################

# å¸¸è§ä¼´å¥æ¨¡å¼åº“
ACCOMPANIMENT_PATTERNS = {
    # 1) Alberti Bass (4/4) => ä¸‹-ä¸Š-ä¸­-ä¸Š
    #   æ¯å°èŠ‚ 4 æ‹, patternä¸­è®°å½•[(æ‹å­, éŸ³ä½)]:
    "alberti_4_4": [
        (0.0, "lowest"),   # æ‹0 ä¸‹
        (1.0, "highest"),  # æ‹1 ä¸Š
        (2.0, "middle"),   # æ‹2 ä¸­
        (3.0, "highest"),  # æ‹3 ä¸Š
    ],

    # 2) Boom-chick (4/4) => ç¬¬0æ‹å¼¹ä½éŸ³, ç¬¬2æ‹å¼¹å’Œå¼¦
    "boomchick_4_4": [
        (0.0, "lowest"),   # Bass
        (2.0, "chord"),    # Chord block
    ],

    # 3) Waltz (3/4) => ç¬¬0æ‹ä½éŸ³, ç¬¬1æ‹å’Œç¬¬2æ‹å¼¹å’Œå¼¦
    "waltz_3_4": [
        (0.0, "lowest"),   # Bass
        (1.0, "chord"),    # Chord
        (2.0, "chord"),    # Chord
    ],

    # 4) Simple arpeggio up (4/4), åœ¨æ¯æ‹æ¼”å¥ chord çš„ ä¸åŒéŸ³
    "arp_up_4_4": [
        (0.0, "lowest"),
        (1.0, "next"),
        (2.0, "next"),
        (3.0, "next"),
    ],

    # 5) Ostinato 16 (4/4), å‡è®¾æ¯å°èŠ‚ 8æ¬¡16åˆ†(åªæ˜¯ç¤ºä¾‹)
    #   ä¾‹å¦‚ patternå†™ [ (0.0, "lowest"), (0.5,"next"), (1.0,"next")... ]
    #   userå¯å†è‡ªå®šä¹‰
    "ostinato_16": [
        (0.0, "lowest"),
        (0.5, "next"),
        (1.0, "middle"),
        (1.5, "next"),
        (2.0, "lowest"),
        (2.5, "next"),
        (3.0, "middle"),
        (3.5, "next"),
    ]
}



def pick_note_from_chord(chord_pcs_sorted, note_position="lowest", lowestBass=36, highestBass=60, 
                         velocity=80, prev_note=None, max_jump=7, cyc_idx_dict=None):
    """
    æ ¹æ® note_position ä» chord_pcs_sorted ä¸­é€‰æ‹©ä¸€ä¸ªéŸ³ç¬¦ï¼Œå¹¶ä¿è¯ç”Ÿæˆçš„éŸ³ç¬¦å›ºå®šåœ¨ä¸€ä¸ªå…«åº¦å†…ï¼Œ
    åŒæ—¶æ£€æŸ¥ä¸å‰ä¸€ä¸ªå·¦å£°éƒ¨éŸ³ç¬¦çš„è·³è·ƒå¹…åº¦ä¸è¶…è¿‡ max_jumpï¼Œé¿å…å‡ºç°çªç„¶çš„é«˜å…«åº¦ã€‚
    note_position å¯ä¸º "lowest", "highest", "middle", "random", "next", "chord" ç­‰ã€‚
    è¿”å› (pitch_list, velocity_list)ã€‚
    
    å‚æ•°è¯´æ˜ï¼š
      - chord_pcs_sorted: å’Œå¼¦éŸ³çš„éŸ³é«˜ï¼ˆä»¥éŸ³é«˜ç±»ï¼Œå³ 0~11 è¡¨ç¤ºéŸ³åï¼‰åˆ—è¡¨ï¼Œå·²æ’åºã€‚
      - lowestBass, highestBass: æŒ‡å®šå…è®¸çš„éŸ³é«˜èŒƒå›´ï¼Œä½†è¿™é‡Œæˆ‘ä»¬å°†éŸ³ç¬¦å›ºå®šåœ¨ä¸€ä¸ªå…«åº¦å†…ï¼Œ
            æ‰€ä»¥æˆ‘ä»¬ä¼šé€‰ç”¨ä¸€ä¸ªåˆé€‚çš„å…«åº¦ï¼ˆä¾‹å¦‚å›ºå®šä¸º octv = 3ï¼‰ã€‚
      - prev_note: å‰ä¸€ä¸ªå·¦å£°éƒ¨éŸ³ç¬¦ï¼Œç”¨äºå¹³æ»‘è·³è·ƒã€‚
      - max_jump: å…è®¸çš„æœ€å¤§è·³è·ƒéŸ³ç¨‹ï¼ˆåŠéŸ³æ•°ï¼‰ã€‚
      - cyc_idx_dict: ç”¨äº "next" æ¨¡å¼å¾ªç¯ç´¢å¼•ã€‚
    """
    # å›ºå®šä½¿ç”¨çš„å…«åº¦ï¼šä¾‹å¦‚è®¾ä¸º 3ï¼Œå³æ„é€ éŸ³ç¬¦æ—¶ç”¨ 12*(3+1)=48 ä½œä¸ºåŸºç¡€ï¼Œè¿™æ ·ç”Ÿæˆçš„éŸ³åœ¨ [48, 59] å†…
    fixed_octv = 3

    def build_candidate(pc):
        octave = (prev_note // 12) if prev_note else fixed_octv + 1  # è®¡ç®—å‡ºä¸€ä¸ªåˆç†çš„å…«åº¦
        return (octave * 12) + (pc % 12)

    def get_base_pitch():
        nonlocal cyc_idx_dict
        # æŒ‰ç…§ note_position é€‰æ‹©ä¸€ä¸ªå’Œå¼¦å†…çš„éŸ³ï¼ˆä»…è¿”å›éŸ³å 0~11ï¼‰
        if not chord_pcs_sorted:
            return 0
        # if note_position == "chord":
             # å¦‚æœæ˜¯ "chord"ï¼Œè¿”å›æ•´ä¸ªåˆ—è¡¨ï¼ˆæ³¨æ„ï¼šè¿™ç§æƒ…å†µåœ¨å·¦å£°éƒ¨ä¸­ä¸€èˆ¬ä¸ç”¨ï¼‰
        #     return chord_pcs_sorted[:]  
        if note_position == "lowest":
            return chord_pcs_sorted[0]
        elif note_position == "highest":
            return chord_pcs_sorted[-1]
        elif note_position == "middle":
            mid = len(chord_pcs_sorted) // 2
            return chord_pcs_sorted[mid]
        elif note_position == "random":
            return random.choice(chord_pcs_sorted)
        elif note_position == "next":
            if cyc_idx_dict is None:
                cyc_idx_dict = {"arp_index": 0}
            if "arp_index" not in cyc_idx_dict:  
                cyc_idx_dict["arp_index"] = 0
            idx = cyc_idx_dict["arp_index"]
            pitch = chord_pcs_sorted[idx % len(chord_pcs_sorted)]
            cyc_idx_dict["arp_index"] = idx + 1
            return pitch
        else:
            return chord_pcs_sorted[0]
    
    candidate = None
    trials = 0
    while trials < 30:
        base_pitch = get_base_pitch()
        cand = build_candidate(base_pitch)
        # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸èŒƒå›´å†…ï¼ˆè™½ç„¶å›ºå®šå…«åº¦ä¸€èˆ¬éƒ½åœ¨èŒƒå›´å†…ï¼‰
        if cand < lowestBass or cand > highestBass:
            trials += 1
            continue

        # æ£€æŸ¥ä¸å‰ä¸€ä¸ªéŸ³ç¬¦çš„è·³è·ƒå¹…åº¦ï¼Œè‹¥å·²æœ‰å‰éŸ³åˆ™è¦æ±‚ä¸è¶…è¿‡ max_jump
        if prev_note is not None:
            if abs(cand - prev_note) > max_jump or (cand // 12 != prev_note // 12):
                trials += 1
                continue

        # å¯åœ¨è¿™é‡ŒåŠ å…¥å…¶å®ƒè§„åˆ™æ£€æŸ¥ï¼Œä¾‹å¦‚é¿å…ä¸å³å£°éƒ¨äº§ç”Ÿå¹³è¡Œå®Œç¾ï¼ˆéœ€ä¼ å…¥å¯¹åº”å³å£°éƒ¨ä¿¡æ¯ï¼‰
        candidate = cand
        break

    # å¦‚æœå¤šæ¬¡å°è¯•åæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å€™é€‰ï¼Œåˆ™é‡‡ç”¨å›é€€ç­–ç•¥ï¼š
    if candidate is None:
        # åœ¨å›ºå®šå…«åº¦å†…ï¼Œå¯¹æ‰€æœ‰å’Œå¼¦éŸ³è®¡ç®—å€™é€‰å€¼
        possible = [build_candidate(pc) for pc in chord_pcs_sorted if lowestBass <= build_candidate(pc) <= highestBass]
        if prev_note is not None and possible:
            # é€‰æ‹©ä¸ prev_note å·®è·æœ€å°çš„å€™é€‰
            # candidate = min(possible, key=lambda x: abs(x - prev_note))
            candidate = min(possible, key=lambda x: (abs(x - prev_note), abs((x // 12) - (prev_note // 12))))
        elif possible:
            candidate = possible[0]
        else:
            # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½ä¸åˆé€‚ï¼Œåˆ™ç›´æ¥è¿”å›å›ºå®šå…«åº¦å†…æœ€ä½çš„éŸ³
            candidate = build_candidate(chord_pcs_sorted[0])
    
    return ([candidate], [velocity])

def generate_accompaniment(
    chord_pcs_sorted, 
    pattern_name,
    start_time=0.0,
    beats_per_bar=4,
    tempo=120,
    lowestBass=36,
    highestBass=60,
    velocityBase=80,
    cyc_idx_dict=None
):
    """
    ç”Ÿæˆä¸€ä¸ªå°èŠ‚çš„ä¼´å¥äº‹ä»¶ï¼ˆå•å£°éƒ¨ï¼‰ï¼Œé‡‡ç”¨æŒ‡å®šçš„ä¼´å¥æ¨¡å¼ã€‚
    å‚æ•°:
      chord_pcs_sorted: å‡åºå’Œå¼¦éŸ³åˆ—è¡¨ (ä¾‹å¦‚ [60,64,67])
      pattern_name: åœ¨ ACCOMPANIMENT_PATTERNS é‡ŒæŸ¥æ‰¾çš„æ¨¡å¼åç§°
      start_time: å°èŠ‚èµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰
      beats_per_bar: æ¯å°èŠ‚æ‹æ•°
      tempo: BPM
      lowestBass, highestBass: éŸ³åŸŸé™åˆ¶
      velocityBase: åŸºç¡€åŠ›åº¦
      cyc_idx_dict: ç”¨äº "next" æ¨¡å¼çš„å¾ªç¯ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
    è¿”å›:
      note_events: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (on_time, off_time, pitch_list, velocity_list)
    """
    if pattern_name not in ACCOMPANIMENT_PATTERNS:
        print(f"Warning: pattern {pattern_name} not found, fallback to 'alberti_4_4'")
        pattern_name = "alberti_4_4"
    pattern = ACCOMPANIMENT_PATTERNS[pattern_name]

    note_events = []
    spb = 60.0 / tempo  # æ¯æ‹ç§’æ•°

    for entry in pattern:
        if len(entry) == 2:
            beat_offset, note_position = entry
            duration_factor = 0.5
        else:
            beat_offset, note_position, duration_factor = entry

        event_on = start_time + beat_offset * spb
        event_off = event_on + duration_factor * spb

        pitches, velocities = pick_note_from_chord(
            chord_pcs_sorted,
            note_position=note_position,
            lowestBass=lowestBass,
            highestBass=highestBass,
            velocity=velocityBase,
            prev_note=None, 
            max_jump=7,
            cyc_idx_dict=cyc_idx_dict
        )
        note_events.append((event_on, event_off, pitches, velocities))
    return note_events

#########################################################
# F. MIDI ä¸ LilyPond è¾…åŠ©å‡½æ•°
#########################################################

def merge_consecutive_notes(notes, epsilon=1e-9):
    sorted_notes = sorted(notes, key=lambda x: x.start)
    merged = []
    for note in sorted_notes:
        if not merged:
            merged.append(note)
        else:
            last = merged[-1]
            if (abs(note.start - last.end) < epsilon and
                note.pitch == last.pitch and
                note.velocity == last.velocity):
                last.end = note.end
            else:
                merged.append(note)
    return merged

def merge_measure(measure):
    merged = []
    if not measure:
        return merged
    current = measure[0]
    count = 1
    for n in measure[1:]:
        if n == current:
            count += 1
        else:
            merged.append((current, count))
            current = n
            count = 1
    merged.append((current, count))
    return merged

def merge_measures(measures):
    return [merge_measure(m) for m in measures]


def get_lilypond_duration(duration_factor):
    """
    å°† duration_factor (å¦‚ 1.5x, 1.0x, 0.75x) è½¬æ¢ä¸º LilyPond çš„éŸ³ç¬¦æ—¶å€¼ (å¦‚ 4, 8, 16)
    """
    if duration_factor >= 1.4:
        return "2"  # äºŒåˆ†éŸ³ç¬¦
    elif duration_factor >= 1.0:
        return "4"  # å››åˆ†éŸ³ç¬¦
    elif duration_factor >= 0.75:
        return "8"  # å…«åˆ†éŸ³ç¬¦
    else:
        return "16"  # åå…­åˆ†éŸ³ç¬¦
       

def duration_token(count, note_index, duration_curve=None):
    """
    æ ¹æ®éŸ³ç¬¦é‡å¤æ¬¡æ•° count å’Œ duration_factor ç”Ÿæˆ LilyPond æ—¶å€¼
    """
    if duration_curve is not None:
        duration_factor = duration_curve[note_index]  # ğŸ¯ ä» `duration_curve` è·å– MIDI æ—¶é•¿ä¿¡æ¯
        lily_duration = get_lilypond_duration(duration_factor)  # ğŸ¯ è½¬æ¢ä¸º LilyPond æ ¼å¼
        return lily_duration
    else:
        mapping = {1: "4", 2: "2", 3: "2.", 4: "1"}
        return mapping.get(count, "4")
    

def midi_to_lily_pitch(midi_val):
    pitch_class = midi_val % 12
    octave = (midi_val // 12) - 1
    names = ["c", "cis", "d", "dis", "e", "f", "fis", "g", "gis", "a", "ais", "b"]
    base = names[pitch_class]
    shift = octave - 3
    if shift > 0:
        base += "'" * shift
    elif shift < 0:
        base += "," * (-shift)
    return base

def measures_to_lily_merged(merged_measures, duration_curve=None):
    lines = []
    note_index = 0

    for measure in merged_measures:
        tokens = []
        for (midiv, count) in measure:
            note_str = midi_to_lily_pitch(midiv) + duration_token(count, note_index, duration_curve=duration_curve)
            tokens.append(note_str)

            # print(f"[LilyPond] Note: {note_str}, Index: {note_index}, MIDI Pitch: {midiv}")
            note_index += count
            
        line = " ".join(tokens) + " |"
        lines.append(line)
    return "\n".join(lines)

def convert_key_for_lily(key):
    mapping = {
        "c#": "cis",
        "d#": "dis",
        "f#": "fis",
        "g#": "gis",
        "a#": "ais",
        "bb": "bes",
        "eb": "ees",
        "gb": "ges",
        "ab": "aes",
        "cb": "ces"
    }
    k = key.lower()
    return mapping.get(k, k)

def convert_ly_to_pdf(ly_file, output_dir=None):
    # å°†è¾“å…¥è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    ly_file_abs = os.path.abspath(ly_file)
    if not os.path.isfile(ly_file_abs):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {ly_file_abs}")
    
    if output_dir:
        # å°†è¾“å‡ºç›®å½•ä¹Ÿè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        output_dir_abs = os.path.abspath(output_dir)
        os.makedirs(output_dir_abs, exist_ok=True)
        base_name = os.path.splitext(os.path.basename(ly_file_abs))[0]
        out_prefix = os.path.join(output_dir_abs, base_name)
        command = ["lilypond", "-fpdf", "-o", out_prefix, ly_file_abs]
        pdf_file = f"{out_prefix}.pdf"
    else:
        base_name = os.path.splitext(os.path.basename(ly_file_abs))[0]
        pdf_file = f"{base_name}.pdf"
        command = ["lilypond", "-fpdf", ly_file_abs]

    try:
        subprocess.run(command, check=True)
        print(f"[INFO] PDFç”ŸæˆæˆåŠŸ: {pdf_file}")
    except subprocess.CalledProcessError:
        print("LilyPondè½¬æ¢å¤±è´¥.")
        return None
    return pdf_file


def from_midi_to_mp3(midi_file):
    """
    å°† MIDI æ–‡ä»¶è½¬æ¢ä¸º WAV/MP3 çš„é€»è¾‘ç¤ºä¾‹
    å‡è®¾ä½ é€šè¿‡ fluidsynth + ffmpeg/ffmpeg æ¥å®ç°
    """
    wav_file = midi_file.replace(".mid", ".wav")
    mp3_file = midi_file.replace(".mid", ".mp3")

    # 1) ç”¨ fluidsynth ç”Ÿæˆ WAV
    subprocess.run([
        "fluidsynth",
        "-ni",
        "/usr/share/sounds/sf2/FluidR3_GM.sf2", 
        midi_file,
        "-F", wav_file,
        "-r", "44100"
    ], check=True)

    # 2) ç”¨ ffmpeg å°† WAV è½¬æˆ MP3
    subprocess.run([
        "ffmpeg",
        "-y",
        "-i", wav_file,
        "-acodec", "libmp3lame",
        mp3_file
    ], check=True)

    print(f"å·²ç”Ÿæˆ MP3 æ–‡ä»¶: {mp3_file}")

def convert_to_type0(pretty_midi_obj, output_filename, left_program_index, right_program_index):
    """
    Convert a PrettyMIDI object to Type 0 while preserving instrument information
    for left and right parts.
    å·¦æ‰‹ä¹å™¨ï¼ˆåç§°ä¸­å« "Left"ï¼‰å°†è®¾ä¸º channel 1 å’Œ left_program_indexï¼Œ
    å³æ‰‹ä¹å™¨ï¼ˆåç§°ä¸­å« "Right"ï¼‰è®¾ä¸º channel 2 å’Œ right_program_index.
    """

    type0_midi = pretty_midi.PrettyMIDI()
    merged_instr = pretty_midi.Instrument(program=0, name="MergedTrack", is_drum=False)

    for instr in pretty_midi_obj.instruments:
        print(f"Copying notes from {instr.name} ({len(instr.notes)} notes)")
        if "Left" in instr.name:
            # ä¿ç•™å·¦æ‰‹ä¹å™¨ä¿¡æ¯
            instr.program = left_program_index
            for note in instr.notes:
                note.channel = 1  # å·¦å£°éƒ¨è®¾ä¸º channel 1
        elif "Right" in instr.name:
            # ä¿ç•™å³æ‰‹ä¹å™¨ä¿¡æ¯
            instr.program = right_program_index
            for note in instr.notes:
                note.channel = 2  # å³å£°éƒ¨è®¾ä¸º channel 2
        else:
            # å¦‚æœä¸æ˜¯å·¦å³å£°éƒ¨ï¼Œé»˜è®¤ä¿æŒä¸å˜
            pass

        merged_instr.notes.extend(instr.notes)

    if len(merged_instr.notes) == 0:
        print("[ERROR] No notes found in converted MIDI!")
    else:
        print(f"[INFO] Successfully merged {len(merged_instr.notes)} notes.")

    merged_instr.notes.sort(key=lambda note: note.start)
    type0_midi.instruments.append(merged_instr)
    type0_midi.write(output_filename)
    print(f"[INFO] Converted to Type 0 MIDI: {output_filename}")

    print(f"Number of tracks: {len(type0_midi.instruments)}")
    for i, instrument in enumerate(type0_midi.instruments):
        print(f"Track {i}: {instrument.name}, Program: {instrument.program}")


def pick_left_voice_note(chord_pcs_sorted, note_position="lowest", 
                         fixed_octv=3,
                         lowestBass=36, highestBass=60, 
                         velocity=80, prev_note=None, max_jump=7, cyc_idx_dict=None,
                         prev_right=None, current_right=None):
    """
    ä» chord_pcs_sorted ä¸­é€‰æ‹©ä¸€ä¸ªå·¦å£°éƒ¨éŸ³ç¬¦ï¼Œä½¿å…¶å›ºå®šåœ¨æŒ‡å®šå…«åº¦å†…ï¼Œ
    å¹¶æ£€æŸ¥ä¸å‰ä¸€ä¸ªå·¦å£°éƒ¨éŸ³ç¬¦çš„è·³è·ƒä¸è¶…è¿‡ max_jumpï¼Œä¸”é¿å…ä¸å³å£°éƒ¨äº§ç”Ÿå¹³è¡Œå®Œç¾ã€‚
    
    chord_pcs_sorted ä¸­çš„æ•°å­—è§†ä¸ºéŸ³åï¼ˆ0ï½11ï¼‰ï¼Œå€™é€‰éŸ³é€šè¿‡å›ºå®šå…«åº¦æ„é€ ä¸ºï¼šbase_octave_base + (pc % 12)
    
    æ–°å¢å‚æ•°ï¼š
      - prev_right: å‰ä¸€æ‹å³å£°éƒ¨éŸ³ç¬¦ï¼ˆè‹¥æœ‰ï¼‰
      - current_right: å½“å‰æ‹å³å£°éƒ¨éŸ³ç¬¦
    è¿”å› (pitch_list, velocity_list)ã€‚
    """
    # å›ºå®šå…«åº¦å†…çš„åŸºç¡€éŸ³ï¼Œä¾‹å¦‚ fixed_octv=3 å¯¹åº” 12*(3+1)=48
    base_octave_base = 12 * (fixed_octv + 1)  # ä¾‹å¦‚ 48

    def build_candidate(pc):
        # å¼ºåˆ¶å°† pc é™å®šåœ¨ 0ï½11 å†…ï¼Œå†åŠ ä¸Šå›ºå®šå…«åº¦åŸºå‡†
        return base_octave_base + pc

    def get_base_pitch():
        if not chord_pcs_sorted:
            return 0
        if note_position == "lowest":
            return chord_pcs_sorted[0]
        elif note_position == "highest":
            return chord_pcs_sorted[-1]
        elif note_position == "middle":
            mid = len(chord_pcs_sorted) // 2
            return chord_pcs_sorted[mid]
        elif note_position == "random":
            return random.choice(chord_pcs_sorted)
        elif note_position == "next":
            if cyc_idx_dict is None:
                cyc_idx_dict = {"arp_index": 0}
            if "arp_index" not in cyc_idx_dict:  
                cyc_idx_dict["arp_index"] = 0
            idx = cyc_idx_dict["arp_index"]
            pitch = chord_pcs_sorted[idx % len(chord_pcs_sorted)]
            cyc_idx_dict["arp_index"] = idx + 1
            return pitch
        else:
            return chord_pcs_sorted[0]
    
    candidate = None
    trials = 0
    while trials < 30:
        base_pitch = get_base_pitch()
        cand = build_candidate(base_pitch)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸èŒƒå›´å†…
        if cand < lowestBass or cand > highestBass:
            trials += 1
            continue

        # æ£€æŸ¥ä¸å‰ä¸€ä¸ªå·¦å£°éƒ¨éŸ³ç¬¦çš„è·³è·ƒå¹…åº¦ï¼Œè‹¥æœ‰å‰éŸ³åˆ™è¦æ±‚ä¸è¶…è¿‡ max_jump
        if prev_note is not None and abs(cand - prev_note) > max_jump:
            trials += 1
            continue

        # æ–°å¢ï¼šé¿å…ä¸å³å£°éƒ¨äº§ç”Ÿå¹³è¡Œå®Œç¾
        # å¦‚æœå­˜åœ¨ä¸Šä¸€æ‹å’Œå½“å‰æ‹å³å£°éƒ¨éŸ³ç¬¦ï¼Œåˆ™åˆ¤æ–­
        if prev_note is not None and prev_right is not None and current_right is not None:
            interval_prev = abs(prev_right - prev_note)
            interval_candidate = abs(current_right - cand)
            if interval_prev in (7, 12) and interval_candidate in (7, 12):
                # äº§ç”Ÿå¹³è¡Œå®Œç¾ï¼Œæ”¾å¼ƒæ­¤å€™é€‰
                trials += 1
                continue

        candidate = cand
        break
    # å¦‚æœ 30 æ¬¡å°è¯•åä»æœªæ‰¾åˆ°åˆé€‚å€™é€‰ï¼Œåˆ™é‡‡ç”¨å›é€€ç­–ç•¥
    if candidate is None:
        possible = [build_candidate(pc) for pc in chord_pcs_sorted]
        # è¿‡æ»¤å‡ºåœ¨å…è®¸èŒƒå›´å†…çš„
        possible = [p for p in possible if lowestBass <= p <= highestBass]
        if prev_note is not None and possible:
            candidate = min(possible, key=lambda x: abs(x - prev_note))
        elif possible:
            candidate = possible[0]
        else:
            candidate = build_candidate(chord_pcs_sorted[0])
    
    return ([candidate], [velocity])

#########################################################
# G. ä¸»å…¥å£: ç”Ÿæˆ MIDI, LY, PDF
#########################################################

def generate_music(
    img_path,
    length=24,
    out_midi="final_out.mid",
    out_ly="final_out.ly",
    out_pdf_dir="outputs",
    method="dual",              # "dual" æˆ– "pattern"
    pattern_name="alberti_4_4",   # å½“ method=="pattern" æ—¶ä½¿ç”¨
    left_program_index=32,        # å·¦å£°éƒ¨ MIDI ç¨‹åºå·ï¼Œé»˜è®¤ 32 (Acoustic Bass)
    right_program_index=0         # å³å£°éƒ¨ MIDI ç¨‹åºå·ï¼Œé»˜è®¤ 0 (Acoustic Grand Piano)
):
    """
    ç”Ÿæˆ MIDIã€LilyPond å’Œ PDF æ–‡ä»¶ã€‚
    
    æµç¨‹ï¼š
      1. ä»å›¾åƒä¸­æå– HSV å‡å€¼å’Œæ·±åº¦ç‰¹å¾ï¼Œç¡®å®šè°ƒæ€§ã€æ¨¡å¼å’ŒèŠ‚å¥é€Ÿåº¦ã€‚
      2. åˆ©ç”¨ Markov é“¾ç”Ÿæˆå’Œå¼¦åºåˆ—ã€‚
      3. æ ¹æ® method é€‰æ‹©ç”Ÿæˆæ–¹å¼ï¼š
         - method="dual": ä½¿ç”¨ generate_dual_voice_measure ç”Ÿæˆå·¦å³å£°éƒ¨ã€‚
         - method="pattern": å³å£°éƒ¨é‡‡ç”¨å¯¹ä½æ¨¡å¼ï¼Œå·¦å£°éƒ¨é‡‡ç”¨ä¼´å¥æ¨¡å¼ï¼ˆgenerate_accompanimentï¼‰ã€‚
      4. å°†ç”Ÿæˆçš„éŸ³ç¬¦å†™å…¥ MIDIï¼Œå¹¶è½¬æ¢ä¸º LilyPond æ–‡æœ¬ï¼Œè°ƒç”¨ LilyPond ç”Ÿæˆ PDFã€‚
    
    å‚æ•°:
      img_path: å›¾åƒæ–‡ä»¶è·¯å¾„
      length: å°èŠ‚æ•°
      out_midi, out_ly, out_pdf_dir: è¾“å‡ºæ–‡ä»¶è·¯å¾„é…ç½®
      method: "dual" æˆ– "pattern"
      pattern_name: å½“ method=="pattern" æ—¶ï¼Œä¸ºå·¦å£°éƒ¨ä¼´å¥é€‰ç”¨çš„æ¨¡å¼åç§°
      left_program_index: å·¦å£°éƒ¨çš„ MIDI ç¨‹åºå·ï¼ˆå¦‚ 32 è¡¨ç¤º Acoustic Bassï¼‰
      right_program_index: å³å£°éƒ¨çš„ MIDI ç¨‹åºå·ï¼ˆå¦‚ 0 è¡¨ç¤º Acoustic Grand Pianoï¼‰
    """
    # a) è¯»å–å›¾åƒä¸ HSV å‡å€¼
    img_bgr = cv2.imread(img_path)
    img_name = os.path.splitext(os.path.basename(img_path))[0]

    if img_bgr is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å– {img_path}")
    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
    h_mean = np.mean(hsv[:, :, 0])
    s_mean = np.mean(hsv[:, :, 1])
    v_mean = np.mean(hsv[:, :, 2])
    
    # b) æå–æ·±åº¦ç‰¹å¾
    model = load_mobilenet_v2()
    # model = load_resnet18_model()
    deep_vec = extract_deep_features_bgr(img_bgr, model)
    # print(deep_vec)
    
    # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒå›¾åƒç”Ÿæˆç›¸åŒéšæœºåºåˆ—
    set_deterministic_seed(deep_vec)
    
    # c) æ ¹æ®æ·±åº¦ç‰¹å¾ç¡®å®šè°ƒæ€§ä¸èŠ‚å¥å‚æ•°
    deep_root, deep_scale, deep_tempo = decide_deep_params(deep_vec)
    
    # ç»“åˆè‰²å½©æ˜ å°„ï¼ˆæ³¨æ„ color_to_tonality_new è¿”å›ä¸‰å…ƒç»„ï¼Œæ­¤å¤„åªå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰
    color_root = color_to_tonality(h_mean, s_mean, v_mean)
    color_root2, _, _ = color_to_tonality_new(h_mean, s_mean, v_mean)
    final_root = color_root2 if random.random() < 0.5 else color_root
    if random.random() < 0.5:
        deep_root = final_root
    print(deep_root, color_root, color_root2)
    
    # d) æ„å»º Markov çŠ¶æ€ä¸å’Œå¼¦åºåˆ—
    states = build_markov_states(deep_scale)
    transition = build_markov_transition(states, s_mean, v_mean)
    chord_seq = generate_chord_sequence(states, transition, length=length)
    print(f"[INFO] chord_seq= {chord_seq}, root= {deep_root}, scale= {deep_scale}, tempo= {deep_tempo}")
    
    # e) æ ¹æ® method é€‰æ‹©ç”Ÿæˆæ–¹å¼
    right_all = []
    left_all  = []
    
    if method == "dual":
        last_interval = None
        hist_notes = (None, None)
        for chord_label in chord_seq:
            chord_pcs = chord_pcs_in_scale(chord_label, deep_root, deep_scale, use_scriabin=False)
            r_ms, l_ms, new_int, new_hist = generate_dual_voice_measure(
                chord_pcs, beats=4,
                last_interval=last_interval, hist_notes=hist_notes
            )
            right_all.append(r_ms)
            left_all.append(l_ms)
            last_interval = new_int
            hist_notes = new_hist
    else:
        # method == "pattern"
        # å³å£°éƒ¨ä»é‡‡ç”¨å¯¹ä½æ¨¡å¼
        
        last_interval_r = None
        hist_notes_r = (None, None)
        spb = 60.0 / deep_tempo
        bar_duration = 4 * spb
        current_time = 0.0
        cyc_dict = {}
        for chord_label in chord_seq:
            chord_pcs = chord_pcs_in_scale(chord_label, deep_root, deep_scale, use_scriabin=False)
            # å³å£°éƒ¨å¯¹ä½ç”Ÿæˆ
            r_ms, _, new_int_r, new_hist_r = generate_dual_voice_measure(
                chord_pcs, beats=4,
                last_interval=last_interval_r,
                hist_notes=hist_notes_r
            )
            right_all.append(r_ms)
            last_interval_r = new_int_r
            hist_notes_r = new_hist_r

            
            # å·¦å£°éƒ¨ä½¿ç”¨ä¼´å¥æ¨¡å¼ç”Ÿæˆ
            events = generate_accompaniment(
                chord_pcs_sorted=chord_pcs,
                pattern_name=pattern_name,
                start_time=current_time,
                beats_per_bar=4,
                tempo=deep_tempo,
                lowestBass=36,
                highestBass=60,
                velocityBase=80,
                cyc_idx_dict=cyc_dict
            )
            current_time += bar_duration
            # å°†æ¯å°èŠ‚ events ç®€åŒ–ä¸º 4 æ‹éŸ³é«˜åˆ—è¡¨ï¼ˆè¿™é‡Œåªå–æ¯æ‹ç¬¬ä¸€ä¸ªäº‹ä»¶çš„éŸ³ç¬¦ï¼‰
            measure_pitch_list = [60, 60, 60, 60]
            for (on_t, off_t, pitches, velocities) in events:
                rel_time = on_t - (current_time - bar_duration)
                beat_idx = int(rel_time // spb)
                if beat_idx < 0:
                    beat_idx = 0
                if beat_idx > 3:
                    beat_idx = 3
                if pitches:
                    measure_pitch_list[beat_idx] = pitches[0]
            left_all.append(measure_pitch_list)
        
        """
        # éå†æ¯ä¸ªå°èŠ‚çš„ right_allï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªæœ‰ 4 ä¸ªæ‹éŸ³é«˜çš„åˆ—è¡¨ï¼‰
        
    
        for measure_right in right_all:
            prev_left = None   # ç”¨äºè®°å½•å‰ä¸€æ‹å·¦å£°éƒ¨éŸ³ç¬¦ï¼Œç”¨äºå¹³æ»‘è¿‡æ¸¡
            measure_left = []   # å­˜æ”¾å½“å‰å°èŠ‚æ¯æ‹çš„å·¦å£°éƒ¨éŸ³ç¬¦
            # å¯¹å½“å‰å°èŠ‚çš„æ¯ä¸€æ‹å¾ªç¯
            for beat_idx, current_right in enumerate(measure_right):
                # å¦‚æœå½“å‰æ‹ä¸æ˜¯ç¬¬ä¸€æ‹ï¼Œåˆ™ä¸Šä¸€æ‹å³å£°éƒ¨éŸ³ç¬¦å–å½“å‰å°èŠ‚ä¸­ä¸Šä¸€æ‹ï¼Œå¦åˆ™ä¸ºç©º
                prev_right = measure_right[beat_idx-1] if beat_idx > 0 else None
                    
                # è°ƒç”¨ pick_left_voice_note ç”Ÿæˆå½“å‰æ‹çš„å·¦å£°éƒ¨éŸ³ç¬¦
                left_voice = pick_left_voice_note(
                    chord_pcs_sorted=chord_pcs,         # å’Œå¼¦éŸ³åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š[0, 4, 7]ï¼‰
                    note_position="lowest",
                    fixed_octv=3,                       # å›ºå®šå…«åº¦ï¼Œä¾‹å¦‚å›ºå®šåœ¨ 48ï½59 ä¹‹é—´
                    lowestBass=36,
                    highestBass=60,
                    velocity=80,
                    prev_note=prev_left,                # ä¸Šä¸€æ‹å·¦å£°éƒ¨éŸ³ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰
                    max_jump=7,
                    prev_right=prev_right,              # ä¸Šä¸€æ‹å³å£°éƒ¨éŸ³ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰
                    current_right=current_right         # å½“å‰æ‹å³å£°éƒ¨éŸ³ç¬¦
                )
                # left_voice è¿”å› ([pitch], [velocity])ï¼Œå–ç¬¬ä¸€ä¸ªéŸ³é«˜ä½œä¸ºå½“å‰æ‹çš„å·¦å£°éƒ¨
                pitch = left_voice[0][0]
                measure_left.append(pitch)
                # æ›´æ–° prev_left ä¸ºå½“å‰æ‹çš„å·¦å£°éƒ¨éŸ³ç¬¦ï¼Œä¾›ä¸‹ä¸€æ‹ä½¿ç”¨
                prev_left = pitch

            left_all.append(measure_left)
            """

    # f) å†™ MIDI æ–‡ä»¶
    pm = pretty_midi.PrettyMIDI()

    right_instr = pretty_midi.Instrument(program=right_program_index, name="RightHand")
    left_instr  = pretty_midi.Instrument(program=left_program_index, name="LeftHand")
    
    MIN_VELOCITY = 40  # æœ€ä½éŸ³é‡ï¼ˆå¯¹åº” `\p`ï¼‰
    MAX_VELOCITY = 100  # æœ€é«˜éŸ³é‡ï¼ˆå¯¹åº” `\f`ï¼‰

    MIN_DURATION_FACTOR = 1.5  # å¼€å§‹å’Œç»“æŸçš„ duration å€æ•°
    NORMAL_DURATION_FACTOR = 1.0  # ä¸­é—´éƒ¨åˆ†çš„ duration
    MAX_DURATION_FACTOR = 1.5  # ç»“å°¾éƒ¨åˆ†çš„ duration å€æ•°


    # è®¡ç®—æ€»éŸ³ç¬¦æ•°
    total_notes = len(right_all) * 4

    # ç”ŸæˆéŸ³é‡å˜åŒ–æ›²çº¿ (æ¸å¼º + ç»´æŒ + æ¸å¼±)
    velocity_curve = []
    duration_curve = []
    for i in range(total_notes):
        if i < total_notes * 0.25:  # ğŸ¼ å‰ 25% æ¸å¼º
            velocity = MIN_VELOCITY + (MAX_VELOCITY - MIN_VELOCITY) * (i / (total_notes * 0.25))
            duration_factor = MIN_DURATION_FACTOR - (MIN_DURATION_FACTOR - NORMAL_DURATION_FACTOR) * (i / (total_notes * 0.25))
        elif i < total_notes * 0.75:  # ğŸ¼ ä¸­é—´ 50% ç»´æŒ
            velocity = MAX_VELOCITY
            duration_factor = NORMAL_DURATION_FACTOR
        else:  # ğŸ¼ å 25% æ¸å¼±
            velocity = MAX_VELOCITY - (MAX_VELOCITY - MIN_VELOCITY) * ((i - total_notes * 0.75) / (total_notes * 0.25))
            duration_factor = NORMAL_DURATION_FACTOR + (MAX_DURATION_FACTOR - NORMAL_DURATION_FACTOR) * ((i - total_notes * 0.75) / (total_notes * 0.25))
        
        velocity_curve.append(int(velocity))  # å–æ•´
        duration_curve.append(duration_factor)

    sec_per_beat = 60.0 / deep_tempo
    current_time = 0.0
 

    # ğŸµ é€ä¸ªéŸ³ç¬¦è®¾ç½® `velocity`
    note_index = 0
    
    sec_per_beat = 60.0 / deep_tempo
    current_time = 0.0

    # å½’ä¸€åŒ– deep_vecï¼Œä½¿å…¶æ˜ å°„åˆ° 0.5x ~ 2x ä¹‹é—´ï¼ˆæ›´åˆç†çš„èŒƒå›´ï¼‰
    normalized_deep_vec = (deep_vec - np.min(deep_vec)) / (np.max(deep_vec) - np.min(deep_vec))  # å½’ä¸€åŒ–åˆ° 0~1
    tempo_modifiers = 0.5 + normalized_deep_vec * 1.5  # å˜æ¢èŒƒå›´ 0.5x ~ 2x

    # ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹å¼ï¼ˆrolling windowï¼‰è®© 1280 ç»´ç‰¹å¾å½±å“æ‰€æœ‰éŸ³ç¬¦
    num_notes = len(right_all) * 4  # è®¡ç®—æ€»éŸ³ç¬¦æ•°
    rolling_window_size = max(1, len(tempo_modifiers) // num_notes)  # è®¡ç®—æ¯ä¸ªéŸ³ç¬¦å¯¹åº”çš„çª—å£å¤§å°

    # è®¡ç®—èŠ‚å¥æƒé‡ï¼Œä½¿å…¶å¹³æ»‘è¿‡æ¸¡
    tempo_modifiers_resampled = np.convolve(tempo_modifiers, np.ones(rolling_window_size) / rolling_window_size, mode='same')

    merged_right = merge_measures(right_all)
    merged_left = merge_measures(left_all)

    note_index = 0
    current_time = 0.0

    for measure_i in range(len(merged_right)):  # âœ… ä½¿ç”¨ merged_right ä»£æ›¿ right_all
        for (r_midi, count) in merged_right[measure_i]:  # âœ… è·å–åˆå¹¶åçš„éŸ³ç¬¦
            start_t = current_time
            base_duration = sec_per_beat  # åŸºç¡€æ—¶é•¿

            # ğŸ¼ å–å½“å‰éŸ³ç¬¦çš„åŠ¨æ€éŸ³é‡å’Œæ—¶é•¿
            velocity = velocity_curve[note_index]
            duration_factor = duration_curve[note_index]
            note_index += count  # âœ… æŒ‰ç…§åˆå¹¶åçš„éŸ³ç¬¦æ•°æ›´æ–°ç´¢å¼•

            # ğŸµ è®¡ç®—éŸ³ç¬¦ç»“æŸæ—¶é—´ï¼ˆåˆå¹¶éŸ³ç¬¦æ—¶è€ƒè™‘ countï¼‰
            end_t = current_time + base_duration * duration_factor * count

            # ğŸ¹ ç”Ÿæˆ MIDI éŸ³ç¬¦
            nr = pretty_midi.Note(velocity=velocity, pitch=r_midi, start=start_t, end=end_t)

            # âœ… æ‰“å° MIDI å³æ‰‹éŸ³ç¬¦ä¿¡æ¯
            # print(f"[MIDI] Right Hand: Pitch={nr.pitch} ({pretty_midi.note_number_to_name(nr.pitch)}), "
            #     f"Velocity={nr.velocity}, Duration={nr.end - nr.start:.3f}")

            right_instr.notes.append(nr)

            current_time = end_t  # âœ… æ›´æ–°æ—¶é—´

    # ğŸ¼ å¤„ç†å·¦æ‰‹å£°éƒ¨
    current_time = 0.0
    note_index = 0

    for measure_i in range(len(merged_left)):  # âœ… ä½¿ç”¨ merged_left ä»£æ›¿ left_all
        for (l_midi, count) in merged_left[measure_i]:  # âœ… è·å–åˆå¹¶åçš„éŸ³ç¬¦
            start_t = current_time
            base_duration = sec_per_beat  # åŸºç¡€æ—¶é•¿

            # ğŸ¼ å–å½“å‰éŸ³ç¬¦çš„åŠ¨æ€éŸ³é‡å’Œæ—¶é•¿
            velocity = velocity_curve[note_index]
            duration_factor = duration_curve[note_index]
            note_index += count  # âœ… æŒ‰ç…§åˆå¹¶åçš„éŸ³ç¬¦æ•°æ›´æ–°ç´¢å¼•

            # ğŸµ è®¡ç®—éŸ³ç¬¦ç»“æŸæ—¶é—´
            end_t = current_time + base_duration * duration_factor * count

            # ğŸ¹ ç”Ÿæˆ MIDI éŸ³ç¬¦
            nl = pretty_midi.Note(velocity=velocity, pitch=l_midi, start=start_t, end=end_t)

            # âœ… æ‰“å° MIDI å·¦æ‰‹éŸ³ç¬¦ä¿¡æ¯
            # print(f"[MIDI] Left Hand: Pitch={nl.pitch} ({pretty_midi.note_number_to_name(nl.pitch)}), "
            #     f"Velocity={nl.velocity}, Duration={nl.end - nl.start:.3f}")

            left_instr.notes.append(nl)

            current_time = end_t  # âœ… æ›´æ–°æ—¶é—´


    right_instr.notes = merge_consecutive_notes(right_instr.notes)
    left_instr.notes = merge_consecutive_notes(left_instr.notes)

    pm.instruments.append(right_instr)
    pm.instruments.append(left_instr)
    pm.write(out_midi)
    from_midi_to_mp3(out_midi)

    convert_to_type0(pm, out_midi, left_program_index, right_program_index)

    # g) ç”Ÿæˆ LilyPond æ–‡æœ¬å¹¶è½¬æ¢ä¸º PDF
    right_lily = measures_to_lily_merged(merged_right, duration_curve=None)
    left_lily = measures_to_lily_merged(merged_left, duration_curve=None)
    

    def attach_absolute_dynamic(measure_list, dynamic):
        """
        ç¡®ä¿ `\p`, `\mp`, `\!` è¿™æ ·çš„ä½ç½®æ ‡è®°ç´§è·ŸéŸ³ç¬¦ï¼Œè€Œä¸æ˜¯å•ç‹¬å­˜åœ¨
        """
        for i, measure in enumerate(measure_list):
            tokens = measure.split()
            for j, token in enumerate(tokens):
                if token[-1].isdigit():  # æ‰¾åˆ°éŸ³ç¬¦ï¼ˆå¦‚ d''4ï¼‰
                    tokens[j] = f"{token}{dynamic}"  # è®©åŠ¨æ€æ ‡è®°ç´§è´´éŸ³ç¬¦
                    measure_list[i] = " ".join(tokens)
                    return measure_list
        return measure_list  # æ²¡æœ‰éŸ³ç¬¦ï¼Œä¸ä¿®æ”¹

    def add_dyn(lily):
        # 1ï¸âƒ£ **æŒ‰å°èŠ‚æ‹†åˆ†**
        measures = [m.strip() for m in lily.split("|") if m.strip()]
        total_measures = len(measures)

        if total_measures < 4:
            return lily  # å°èŠ‚å¤ªå°‘ï¼Œä¸åšåŠ¨æ€å¤„ç†

        # 2ï¸âƒ£ **è®¡ç®—åˆ†æ®µç´¢å¼•**
        seg1_count = max(1, total_measures // 4)  # å‰ 25%
        seg3_count = max(1, total_measures // 4)  # å 25%
        seg2_count = total_measures - (seg1_count + seg3_count)  # ä¸­é—´ 50%

        # 3ï¸âƒ£ **åˆ’åˆ†å°èŠ‚**
        seg1_measures = measures[:seg1_count]      # å‰ 25%
        seg2_measures = measures[seg1_count: seg1_count + seg2_count]  # ä¸­é—´ 50%
        seg3_measures = measures[-seg3_count:]     # å 25%


        # 4ï¸âƒ£ **ä¿®æ­£ `\p` `\mp` ç»‘å®šéŸ³ç¬¦**
        if seg1_measures:
            seg1_measures = attach_absolute_dynamic(seg1_measures, "\\p")  # `\p` ç»‘å®šéŸ³ç¬¦
            seg1_measures = attach_absolute_dynamic(seg1_measures, "\\<")  # `\<` ç»‘å®šéŸ³ç¬¦

        if seg2_measures:
            seg2_measures = attach_absolute_dynamic(seg2_measures, "\\!")  # `\!` ç»‘å®šéŸ³ç¬¦

        if seg3_measures:
            seg3_measures = attach_absolute_dynamic(seg3_measures, "\>")  # `\>` ç»‘å®šéŸ³ç¬¦
            seg3_measures = attach_absolute_dynamic(seg3_measures, " \! \\mp")  # `\mp` ç»‘å®šéŸ³ç¬¦

        # 5ï¸âƒ£ **é‡æ–°æ‹¼æ¥å°èŠ‚**
        final_lily = " | ".join(seg1_measures + seg2_measures + seg3_measures) + " |"

        return final_lily
    
    final_right_lily = add_dyn(right_lily)
    final_left_lily = add_dyn(left_lily)

    # print(len(final_right_lily), len(final_left_lily))


    lily_root = convert_key_for_lily(deep_root)

    lily_content = f"""\\version "2.24.1"
    \\header {{
        title = "{img_name}"
        % composer = "Yao."
    }}
    \\score {{
        \\new PianoStaff <<
            \\new Staff = "right" {{
                \\clef treble
                \\key {lily_root} \\{deep_scale}
                % \\tempo 4={deep_tempo}
                {final_right_lily}
                \\bar "|."
            }}
            \\new Staff = "left" {{
                \\clef bass
                \\key {lily_root} \\{deep_scale}
                {final_left_lily}
                \\bar "|."
            }}
        >>
        \\layout {{}}
        \\midi {{}}
    }}
    """
    with open(out_ly, "w", encoding="utf-8") as f:
        f.write(lily_content)
    print("[INFO] LilyPond å†™å…¥:", out_ly)
    
    pdf_file = convert_ly_to_pdf(out_ly, output_dir=out_pdf_dir)
    if pdf_file:
        print("[INFO] ç”ŸæˆPDF:", pdf_file)

    png_file = pdf_file[:-4] + ".png"
    subprocess.run(
        ["convert", 
        "-density", "300", 
        pdf_file, 
        png_file
    ],check=True)
    if png_file:
        print("[INFO] ç”ŸæˆPNG:", png_file)


    
if __name__ == "__main__":

    import argparse
    parser = argparse.ArgumentParser(
        description="Generate music from image using deep learning-based algorithm."
    )
    parser.add_argument("--img_path", type=str, required=True, help="Path to the input image.")
    parser.add_argument("--length", type=int, default=24, help="Number of measures (chord sequence length).")
    parser.add_argument("--method", type=str, choices=["dual", "pattern"], default="dual",
                        help="Generation method: 'dual' for dual-voice counterpoint, 'pattern' for accompaniment pattern mode.")
    parser.add_argument("--pattern_name", type=str, default="alberti_4_4",
                        help="When method is 'pattern', the accompaniment pattern name to use.")
    parser.add_argument("--left_program_index", type=int, default=32,
                        help="MIDI program index for the left-hand instrument (e.g., 32 for Acoustic Bass).")
    parser.add_argument("--right_program_index", type=int, default=0,
                        help="MIDI program index for the right-hand instrument (e.g., 0 for Acoustic Grand Piano).")
    parser.add_argument("--out_midi", type=str, default="final_out.mid", help="Output MIDI file path.")
    parser.add_argument("--out_ly", type=str, default="final_out.ly", help="Output LilyPond file path.")
    parser.add_argument("--out_pdf_dir", type=str, default="outputs", help="Output directory for the PDF file.")
    
    args = parser.parse_args()
    img_name = args.img_path.split('/')[-1].split('.')[0]
    
    generate_music(
        img_path=args.img_path,
        length=args.length,
        out_midi=os.path.join(args.out_pdf_dir, f"{img_name}.mid"),
        out_ly=os.path.join(args.out_pdf_dir, f"{img_name}.ly"),
        out_pdf_dir=args.out_pdf_dir,
        method=args.method,
        pattern_name=args.pattern_name,
        left_program_index=args.left_program_index,
        right_program_index=args.right_program_index
    )
    
    print("[DONE]")